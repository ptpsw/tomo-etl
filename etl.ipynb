{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "541a0881",
   "metadata": {},
   "source": [
    "# ETL Process Tomography Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a78bd-32b0-4d63-b5d4-94f06c266f4c",
   "metadata": {},
   "source": [
    "Tomography data are available in two folders:\n",
    "- data/L2 -> STD, SNR, sincos\n",
    "- data/L3 -> temperature and current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "971c1a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pymysql.cursors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from sql_queries import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14000501-7f83-462e-9136-bc34dc5581ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = pymysql.connect(host='localhost', user='root', password='mypassword', database='tomo')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4c5141-1ee7-4c39-a6fc-15eb9096d184",
   "metadata": {},
   "source": [
    "If you want to see all the df, you can set max_rows setting as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d90d8f1-9782-4bf1-be5b-b01de566e31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 100)\n",
    "# pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569698d-c5bd-4ac4-b797-47daa63cdcbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stations\n",
    "\n",
    "Because station information is not provided from the data in the folder, we need to assign station information manually.\n",
    "\n",
    "Station is grouped in a region which identified from the ID, for example GI-01 and GI-02 are both Gili Iyang stations. New stations need to be assigned this ID which agreed by the team.\n",
    "\n",
    "As mentioned before, data is separated into two folder: L2 and L3. So each station is also mapped to named folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd130aa7-aac2-40d5-9eb2-9fdccd65a8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>l2_folder</th>\n",
       "      <th>l3_folder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GI-01</td>\n",
       "      <td>Gili Iyang 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GI_01</td>\n",
       "      <td>GI01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GI-02</td>\n",
       "      <td>Gili Iyang 2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GI_02</td>\n",
       "      <td>GI02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GI-03</td>\n",
       "      <td>Gili Iyang 3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GI_03</td>\n",
       "      <td>GI03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GI-04</td>\n",
       "      <td>Gili Iyang 4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GI_04</td>\n",
       "      <td>GI04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  station_id          name  lat  lon l2_folder l3_folder\n",
       "0      GI-01  Gili Iyang 1  NaN  NaN     GI_01      GI01\n",
       "1      GI-02  Gili Iyang 2  NaN  NaN     GI_02      GI02\n",
       "2      GI-03  Gili Iyang 3  NaN  NaN     GI_03      GI03\n",
       "3      GI-04  Gili Iyang 4  NaN  NaN     GI_04      GI04"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_df = pd.read_csv('stations.csv')\n",
    "stations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1121dd42-9f1f-4f7a-972e-c3532f6da0c4",
   "metadata": {},
   "source": [
    "Insert into Stations table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04ff7c1c-f343-4b3e-8247-6715c4a0f8a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace np.nan with None for MySQL insert to work\n",
    "stations_df_2 = stations_df.replace({np.nan: None})\n",
    "\n",
    "for index, row in stations_df_2.iterrows():\n",
    "    cur.execute(stations_table_insert, list(row))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc5da7-c99a-4ae1-ac69-2d86c08cceb3",
   "metadata": {},
   "source": [
    "Run `test.ipynb` to see the inserted stations data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d046b55-5ce0-42d9-9eda-48c8e32f20bd",
   "metadata": {},
   "source": [
    "# Process L2 Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2279f",
   "metadata": {},
   "source": [
    "First create regex for finding all csv files with specified format.\n",
    "\n",
    "- sincos_file_format = '1210619_04_30_00.csv'\n",
    "- snr_file_format = '1210618_20_40_00_SNR.csv'\n",
    "- stack_file_format = '1210619_04_10_00to1210619_00_00_00.csv'\n",
    "- std_file_format = '1210619_04_20_00to1210619_00_00_00_std.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9223e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_date      = '([0-9]{6}_[0-9]{2}_[0-9]{2}_[0-9]{2})'\n",
    "regex_sincos    = re.compile('(?P<station>[0-9])' + regex_date + '.csv')\n",
    "regex_snr       = re.compile('(?P<station>[0-9])' + regex_date + '_SNR.csv')\n",
    "regex_stack     = re.compile('(?P<station>[0-9])' + regex_date + 'to' +\n",
    "                             '(?P<station2>[0-9])' + regex_date + '.csv')\n",
    "regex_std       = re.compile('(?P<station>[0-9])' + regex_date + 'to' +\n",
    "                             '(?P<station2>[0-9])' + regex_date + '_std.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f72c85ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(filepath, regex):\n",
    "    all_files = []\n",
    "    count_match = 0\n",
    "    count_not_match = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        glob_files = glob.glob(os.path.join(root, '*.csv'))\n",
    "        for f in glob_files:\n",
    "            head, tail = os.path.split(f)\n",
    "            if(regex.match(tail)):\n",
    "                all_files.append(os.path.abspath(f))\n",
    "                count_match +=1\n",
    "            else:\n",
    "                count_not_match+=1\n",
    "                \n",
    "    print('Found {} matching files from total {} files'.format(count_match, count_match+count_not_match))\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20738117-33bd-4dbf-b591-09fda615e7ca",
   "metadata": {},
   "source": [
    "### Get daily Max SNR/STD files from all stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20a49d3-c831-4a18-a1bd-c7bbe047d60f",
   "metadata": {},
   "source": [
    "Get all std files from all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b1670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "std_files = get_files('data/historical_all_stations', regex_std)\n",
    "# std_files = get_files('data/historical_data/L2', regex_std)\n",
    "\n",
    "std_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4efec00-cc5e-4f81-b58c-62e77359ae5a",
   "metadata": {},
   "source": [
    "Check the std files headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9527d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_df = pd.read_csv(std_files[0], sep=' ')\n",
    "std_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a18524e-a538-43fc-ab60-b75514e080a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_df_2 = pd.read_csv(std_files[134], sep=' ')\n",
    "std_df_2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb9ac9a-5b67-4c74-bd08-075a0914c1cb",
   "metadata": {},
   "source": [
    "Apparently after there are two different type of std csv as we will check further below. \n",
    "The one that are we are using is those with `Max_SNR` and `day_in_decimal` header.\n",
    "\n",
    "Now we are going to build csv files dataframe to easily query which file to ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10330955-772f-478c-aba3-2dc11a7a4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_station_folder(filepath):\n",
    "    return os.path.basename(os.path.dirname(filepath))\n",
    "def get_std_datetime(filename):\n",
    "    return datetime.strptime(filename[1:16], '%y%m%d_%H_%M_%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878003d-f4cb-4cb8-8dd4-68982e0399e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "header_df = pd.DataFrame(columns=['station', 'stfolder', 'date', 'datetime', 'file', 'columns', 'nrows', 'filepath'])\n",
    "for file in std_files:\n",
    "    std_df = pd.read_csv(file, sep=' ')\n",
    "    filename = os.path.split(file)[1]\n",
    "    columns_list = list(std_df)\n",
    "    header_df = header_df.append(\n",
    "        {\n",
    "            'file': filename, \n",
    "            'columns':''.join(map(str, columns_list)),\n",
    "            'nrows': len(std_df.index),\n",
    "            'datetime': get_std_datetime(filename),\n",
    "            'date': get_std_datetime(filename).date(),\n",
    "            'stfolder': get_std_station_folder(file),\n",
    "            'filepath': file\n",
    "        }, ignore_index=True)\n",
    "\n",
    "header_df['columns'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96925863-7756-48f7-9c53-b91d9138068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_files_df = header_df.loc[header_df['columns'] == 'Max_SNRday_in_decimal'].copy()\n",
    "std_files_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf358ea4-a2b6-477c-af60-0167b03fa61d",
   "metadata": {},
   "source": [
    "Get all maximum number of rows per day as the main **daily aggregated data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbb00c-0ee3-4bba-afe4-36c807c60978",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = std_files_df.groupby([\"stfolder\", \"date\"])['nrows'].transform(max) == std_files_df['nrows']\n",
    "std_files_df[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc8421-b9e6-4860-ac8b-693d3a737efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows max() of each column which return the largest nrows but it also return the \n",
    "# \"latest\" file name which may not be the one with largest nrows. Use this to get the latest data per day. \n",
    "std_files_df = std_files_df.groupby([\"stfolder\", \"date\"], as_index=False).max()\n",
    "std_files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17077447-2e76-4230-8cde-e731541ba4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to check nrows of each file on a given stfolder and date\n",
    "# std_files_df.loc[(std_files_df['stfolder']=='GI_01') & (std_files_df['date']==pd.to_datetime('2021-06-22').date())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cb1c46-b4bf-4449-aa5f-4a1f0406726d",
   "metadata": {},
   "source": [
    "### Process std file\n",
    "\n",
    "Test with one of the file as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c104d-ce42-40d5-9b39-8bf5526bd43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_file = std_files_df.iloc[3].filepath\n",
    "std_date = std_files_df.iloc[3].datetime\n",
    "\n",
    "cur.execute(get_station_id_l2_sql, (std_files_df.iloc[3].stfolder))\n",
    "std_station, = cur.fetchone()\n",
    "\n",
    "std_data_df = pd.read_csv(std_file, sep=' ')\n",
    "std_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f9517-3bdd-455d-a671-bdb7cbbd6000",
   "metadata": {},
   "source": [
    "NOTE: day_in_decimal is time in UTC parsed as 'x.y' where x is the day in given month (from filename) and y time in 24 hour.\n",
    "eg: For file '1210623_23_40_00to1210623_00_00_00_std.csv' and day_in_decimal 23.979167, it means \n",
    "the data date is 23 June 2021 and the time is 0.979167*24=23.500008 in hour or 23:30:00.0288 \n",
    "\n",
    "But due to some issues, some early files might have differences between 'x' and the date from the filename. In that case get day from the file name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb53d1ee-c316-4c7c-a869-2d6f54770d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_decimal_to_timestamp(datetime, day_decimal_ser):\n",
    "    timestamp_ser = pd.to_datetime(datetime.date()).value + day_decimal_ser%1*86400*1e9\n",
    "    return pd.to_datetime(timestamp_ser, format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a5b591-42d0-4694-8d78-fd844668e2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_data_df['timestamp'] = day_decimal_to_timestamp(std_date, std_data_df['day_in_decimal'])\n",
    "std_data_df['station'] = std_station\n",
    "std_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc58da-3bfe-463f-9053-113d1591682a",
   "metadata": {},
   "source": [
    "Insert into Max SNR/STD table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629b09b5-9bbf-4099-bae3-32332eabc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in std_data_df[['station', 'timestamp', 'Max_SNR']].iterrows():\n",
    "    cur.execute(max_snr_table_insert, list(row))\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642861b8-2a6b-4cca-b634-300321c944bf",
   "metadata": {},
   "source": [
    "If all ok then repeat to all the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f498cd8-10e3-4867-abb7-ae2b6d16672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, file in std_files_df.iterrows():\n",
    "    std_file = file.filepath\n",
    "    std_date = file.datetime\n",
    "    std_station = get_station_id(cur, file.stfolder)\n",
    "    std_data_df = pd.read_csv(std_file, sep=' ')\n",
    "    std_data_df['timestamp'] = day_decimal_to_timestamp(std_date, std_data_df['day_in_decimal'])\n",
    "    std_data_df['station'] = std_station\n",
    "    for index, row in std_data_df[['station', 'timestamp', 'Max_SNR']].iterrows():\n",
    "        cur.execute(max_snr_table_insert, list(row))\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcaf684-d770-471e-bbdb-edb64ddd2bec",
   "metadata": {},
   "source": [
    "# Process L3 Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140064bd-54c3-4861-aba2-e1a5e5494211",
   "metadata": {},
   "source": [
    "Next, we want to create regex for L3 csv files with specified format.\n",
    "\n",
    "- current_file_format = `Curr_G101-GI02.csv`\n",
    "- temperature_file_format = `Temp_G101-GI03.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9d38aa3-ca75-4671-a14b-48fe0d750308",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_current = re.compile('Curr_(?P<src_station>[0-9A-Za-z]{4})-(?P<dest_station>[0-9A-Za-z]{4}).csv')\n",
    "regex_temp    = re.compile('Temp_(?P<src_station>[0-9A-Za-z]{4})-(?P<dest_station>[0-9A-Za-z]{4}).csv')\n",
    "regex_test    = re.compile('.*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb65831c-daee-4d38-8560-483e3833c886",
   "metadata": {},
   "source": [
    "### Get All Temperature files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7334926a-5a21-4907-b557-794fbe017c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 matching files from total 39 files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\workspace\\\\tomo\\\\tomo-etl\\\\data\\\\historical_data\\\\L3\\\\st00\\\\Temp_GI01-GI02.csv',\n",
       " 'C:\\\\workspace\\\\tomo\\\\tomo-etl\\\\data\\\\historical_data\\\\L3\\\\st00\\\\Temp_GI01-GI03.csv',\n",
       " 'C:\\\\workspace\\\\tomo\\\\tomo-etl\\\\data\\\\historical_data\\\\L3\\\\st00\\\\Temp_GI01-GI04.csv',\n",
       " 'C:\\\\workspace\\\\tomo\\\\tomo-etl\\\\data\\\\historical_data\\\\L3\\\\st02\\\\Temp_GI03-GI04.csv']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_files = get_files('data/historical_data/L3', regex_temp)\n",
    "temp_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edc3e28-ba0f-4958-8e1c-dc8367ab030b",
   "metadata": {},
   "source": [
    "Take a look on one of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80a1292f-7632-48c4-8a6a-47b44cb2e604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_in_decimal</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.208333</td>\n",
       "      <td>29.238612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24.201389</td>\n",
       "      <td>29.219702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.145833</td>\n",
       "      <td>29.224371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.138889</td>\n",
       "      <td>29.223039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day_in_decimal  temperature\n",
       "0       24.208333    29.238612\n",
       "1       24.201389    29.219702\n",
       "2       24.145833    29.224371\n",
       "3       24.138889    29.223039"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = pd.read_csv(temp_files[0], usecols=['Date_In_Decimal', 'degree_(C)'])\n",
    "temp_df.columns = ['day_in_decimal', 'temperature']\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dee3fd-0eac-4775-832a-9b56025b2669",
   "metadata": {},
   "source": [
    "### Process Temperature files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "347254d5-a6a4-4c7d-b52d-fc4dd031bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_l3_file(file, regex):\n",
    "    head, tail = os.path.split(file)\n",
    "    m = regex.match(tail)\n",
    "    return m.group('src_station'), m.group('dest_station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa5fd9a-8f2c-4342-b2dc-08d78117378c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_decimal_to_timestamp(datetime, day_decimal_ser):\n",
    "    timestamp_ser = pd.to_datetime(datetime.date()).value + day_decimal_ser%1*86400*1e9\n",
    "    return pd.to_datetime(timestamp_ser, format='%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be160160-5b38-4f7b-84e2-561ea158985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing temperature files: 1/4 files\n",
      "Processing temperature files: 2/4 files\n",
      "Processing temperature files: 3/4 files\n",
      "Processing temperature files: 4/4 files\n"
     ]
    }
   ],
   "source": [
    "files_count = 0\n",
    "total_count = len(temp_files)\n",
    "for file in temp_files:\n",
    "    print(f'Processing temperature files: {files_count+1}/{total_count} files')\n",
    "    src_station, dest_station = parse_l3_file(file, regex_temp)\n",
    "    \n",
    "    # Query station id from parsed filename and check file naming consistency\n",
    "    cur.execute(get_station_id_l3_sql, (src_station))\n",
    "    src_station_result = cur.fetchone()\n",
    "    cur.execute(get_station_id_l3_sql, (dest_station))\n",
    "    dest_station_result = cur.fetchone()\n",
    "    \n",
    "    if ((src_station_result is None) or (dest_station_result is None)):\n",
    "        raise ValueError('Unknown station, please check your L3 file naming')\n",
    "    else:\n",
    "        src_station_id, = src_station_result\n",
    "        dest_station_id, = dest_station_result\n",
    "        \n",
    "        \n",
    "    # Get or create new station link\n",
    "    cur.execute(get_station_link_sql, (src_station_id, dest_station_id, \n",
    "                                       dest_station_id, src_station_id))\n",
    "    link_result = cur.fetchone()\n",
    "        \n",
    "    if link_result is None:\n",
    "        link_name = f'{src_station_id}_{dest_station_id}'\n",
    "        cur.execute(station_link_id_insert, (link_name, src_station_id, dest_station_id))\n",
    "        conn.commit()\n",
    "        print(f'Created new station link {link_name}')\n",
    "        \n",
    "    cur.execute(get_station_link_sql, (src_station_id, dest_station_id, \n",
    "                                       dest_station_id, src_station_id))\n",
    "    link_id, link_name, src_id, dest_id = cur.fetchone()\n",
    "    \n",
    "    # Insert temperature data\n",
    "    temp_df = pd.read_csv(file, usecols=['Date_In_Decimal', 'degree_(C)'])\n",
    "    temp_df.columns = ['day_in_decimal', 'temperature']\n",
    "    temp_df['link_id'] = link_id\n",
    "    # TODO confirm how to get the day in decimal day\n",
    "    temp_df['timestamp'] = pd.to_datetime(\n",
    "        pd.to_datetime('20210531', format=\"%Y%m%d\").value + temp_df['day_in_decimal']*86400*1e9, \n",
    "        format='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    for index, row in temp_df[['link_id', 'timestamp', 'temperature']].iterrows():\n",
    "        cur.execute(temp_table_insert, list(row))\n",
    "        conn.commit()\n",
    "    \n",
    "    files_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283311d7-300a-4bbf-b733-bf50d305893f",
   "metadata": {},
   "source": [
    "# Close connection to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091c07ba-bb8e-48e9-b6b2-50e628b2b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f4843-5b75-4407-8575-9d57afd1c7b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d28c3cfa50f6bcb2bc350461f1eb37409f1dc6f49bba7b67bf03761ff1cd8671"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
